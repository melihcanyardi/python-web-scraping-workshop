{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4f609a-45f9-41ac-84a7-d3c585ec6b51",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping with Python for Social Scientists\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "<b>Melih Can Yardı</b><br>\n",
    "Researcher @Politus Analytics<br>\n",
    "24.12.2024\n",
    "</div>\n",
    "\n",
    "## Table of Contents\n",
    "### 1. Introduction to Web Scraping\n",
    "- What is web scraping?\n",
    "- Ethical considerations and legal aspects (`robots.txt`, Terms of Service).\n",
    "- Differences between APIs and scraping.\n",
    "\n",
    "### 2. Understanding the Web\n",
    "- Basic structure of a webpage: HTML, CSS, JavaScript.\n",
    "- Deep Dive into HTML\n",
    "\n",
    "### 3. Static Web Scraping\n",
    "- Making HTTP requests using `requests`.\n",
    "    - HTTP status codes: `200`, `404`, `500`\n",
    "- Parsing HTML with `Beautiful Soup`.\n",
    "    - Selecting elements with Beautiful Soup:\n",
    "      - `find`\n",
    "      - `find_all`\n",
    "- **Example 1:** Collect information about countries\n",
    "\n",
    "### 4. Dynamic Web Scraping\n",
    "- Introduction to JavaScript-rendered pages.\n",
    "- Using Selenium for web scraping.\n",
    "- Setting up a Chrome browser.\n",
    "- **Example 2:** Collect Oscar Winning Films\n",
    "- **Example 3:** Collect Hockey Teams\n",
    "\n",
    "### 5. Q&A and Further Learning\n",
    "- Q&A session.\n",
    "- Sharing resources for further learning.\n",
    "\n",
    "### Further Resources:\n",
    "- [Data Collection from the Web using APIs and web scraping techniques](https://github.com/ahurriyetoglu/text-processing-for-social-sciences/blob/main/practical_sessions/1-Data_Collection/Practical_Session-Data_Collection.ipynb)\n",
    "- [Requests Documentation](https://requests.readthedocs.io/en/latest/)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Selenium with Python Documentation](https://selenium-python.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b972113-7bb6-4273-8905-7cca736c3e52",
   "metadata": {},
   "source": [
    "# 1. Introduction to Web Scraping\n",
    "\n",
    "---\n",
    "\n",
    "## What is Web Scraping?\n",
    "- **Web scraping** is the automated process of extracting data from websites. It involves sending `requests` to a webpage, parsing the `HTML` content, and extracting the desired information.\n",
    "---\n",
    "\n",
    "## Ethical Considerations and Legal Aspects\n",
    "As researchers, it is essential to ensure that scraping is done responsibly and within legal boundaries.\n",
    "\n",
    "- **Terms of Service (ToS)**: Many websites include restrictions against scraping in their ToS. Violating these could lead to legal consequences. Examples:\n",
    "    - [X's Terms of Service](https://x.com/en/tos)\n",
    "    - [Reddit's User Agreement](https://redditinc.com/policies/user-agreement)\n",
    "- **robots.txt**:\n",
    "    - A `robots.txt` file specifies parts of a website that are restricted for bots.\n",
    "    - Always check this file to determine which pages can be scraped ethically and legally.\n",
    "    - Example of a `robots.txt` file:\n",
    "      ```\n",
    "      User-agent: *\n",
    "      Disallow: /private/\n",
    "      Allow: /public/\n",
    "      ```\n",
    "    - Examples:\n",
    "        - [Twitter's robots.txt file](https://x.com/robots.txt)\n",
    "        - [Reddit's robots.txt file](https://www.reddit.com/robots.txt)\n",
    "    - Note: `robots.txt` is not legally binding but indicates the site owner's preference.\n",
    "\n",
    "- **Best Practices**:\n",
    "  - Use scraping tools responsibly, adhering to the website's policies.\n",
    "  - Attribute the source of the data when sharing or publishing your research.\n",
    "  - Clearly indicate how the collected data will be used, especially for research purposes.\n",
    "\n",
    "---\n",
    "\n",
    "## Differences Between APIs and Scraping\n",
    "Web scraping and APIs are both methods for collecting data, but they differ in approach, use cases, and limitations.\n",
    "\n",
    "- **Web Scraping**:\n",
    "  - Involves extracting data from HTML content on webpages.\n",
    "  - Requires navigating and parsing the structure of the website (e.g., HTML, CSS).\n",
    "  - Useful when an API is unavailable or provides limited access to the required data.\n",
    "\n",
    "- **APIs (Application Programming Interfaces)**:\n",
    "  - APIs are provided by websites or services to allow developers to access structured data directly.\n",
    "  - Easier and more reliable than scraping (no need to parse HTML).\n",
    "  - Often comes with rate limits and requires an API key or authentication.\n",
    "  - Examples: [Twitter API Documentation](https://developer.x.com/en/docs/x-api), [Reddit API](https://www.reddit.com/dev/api/).\n",
    "\n",
    "- **Comparison Table**:\n",
    "  | Feature                | Web Scraping                 | APIs                        |\n",
    "  |------------------------|------------------------------|-----------------------------|\n",
    "  | **Ease of Use**        | More complex (requires HTML parsing) | Easier (structured data provided) |\n",
    "  | **Data Access**        | Any public webpage           | Limited to API endpoints    |\n",
    "  | **Rate Limits**        | Depends on owners' preferences | Enforced by API providers  |\n",
    "  | **Reliability**        | Prone to break if site changes | More stable                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d242bc2-f8a1-4e67-9e1a-feb73f20d0f3",
   "metadata": {},
   "source": [
    "# 2. Understanding the Web\n",
    "\n",
    "The basic building blocks of a webpage: **HTML**, **CSS**, and **JavaScript**.\n",
    "\n",
    "---\n",
    "\n",
    "## HTML (HyperText Markup Language)\n",
    "- **Definition**: HTML provides the structure and content of a webpage.\n",
    "- **Example**:\n",
    "```html\n",
    "<h1>Welcome to My Page</h1>\n",
    "<p>This is a paragraph of text.</p>\n",
    "<a href=\"https://example.com\">Click here to visit Example</a>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## CSS (Cascading Style Sheets)\n",
    "- **Definition**: CSS controls the appearance and layout of HTML elements.\n",
    "- **Example**:\n",
    "```html\n",
    "<style>\n",
    "  h1 {\n",
    "    color: blue;\n",
    "    font-size: 24px;\n",
    "  }\n",
    "</style>\n",
    "<h1>Styled Heading</h1>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## JavaScript\n",
    "- **Definition**: JavaScript adds interactivity and dynamic behavior to webpages.\n",
    "- **Example**:\n",
    "```html\n",
    "<script>\n",
    "  function greet() {\n",
    "    alert(\"Hello, World!\");\n",
    "  }\n",
    "</script>\n",
    "<button onclick=\"greet()\">Click Me</button>\n",
    "```\n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "## Deep Dive into HTML\n",
    "Since we will parse HTML, understanding its elements is crucial for web scraping.\n",
    "\n",
    "### Common HTML Tags\n",
    "- `<div>`: A container for other elements, often used for layout and grouping.\n",
    "- `<span>`: Inline container for text or other elements.\n",
    "- `<a>`: Defines a hyperlink (e.g., `<a href=\"https://example.com\">Link</a>`).\n",
    "- `<img>`: Embeds images (e.g., `<img src=\"image.jpg\" alt=\"Description\">`).\n",
    "- `<table>`, `<tr>`, `<td>`: Used for tabular data.\n",
    "- `<ul>`, `<ol>`, `<li>`: Defines lists.\n",
    "\n",
    "---\n",
    "\n",
    "### HTML Attributes\n",
    "Attributes provide additional information about elements and are key for selecting specific parts of a page.\n",
    "- **`id`**: Unique identifier for an element (e.g., `<div id=\"header\">`).\n",
    "- **`class`**: Groups elements for styling or selection (e.g., `<p class=\"text\">`).\n",
    "- **`href`**: URL for links (e.g., `<a href=\"https://example.com\">Link</a>`).\n",
    "- **`src`**: Source for images or scripts (e.g., `<img src=\"image.jpg\">`).\n",
    "- **`alt`**: Alternative text for images (e.g., `<img src=\"image.jpg\" alt=\"Description\">`).\n",
    "\n",
    "---\n",
    "\n",
    "### HTML Hierarchy and Nesting\n",
    "HTML documents are structured as a hierarchy. Elements can be nested within one another, creating a tree-like structure.\n",
    "- Example:\n",
    "  ```html\n",
    "  <div class=\"container\">\n",
    "    <h1>Title</h1>\n",
    "    <p>This is a paragraph inside the container.</p>\n",
    "  </div>\n",
    "  ```\n",
    "- **Parent**: `<div>` is the parent of `<h1>` and `<p>`.\n",
    "- **Child**: `<h1>` and `<p>` are children of `<div>`.\n",
    "- **Siblings**: `<h1>` and `<p>` are siblings.\n",
    "\n",
    "### HTML Selectors\n",
    "Selectors are used to locate and extract elements from a webpage.\n",
    "\n",
    "- **By ID**: Use the unique `id` of an element.\n",
    "  - Example: `#header` selects an element like `<div id=\"header\">`.\n",
    "- **By Class**: Use the `class` attribute to select groups of elements.\n",
    "  - Example: `.text` selects all elements with `class=\"text\"`.\n",
    "- **By Tag Name**: Select all elements of a specific type.\n",
    "  - Example: `p` selects all `<p>` (paragraph) tags.\n",
    "- **Combining Selectors**: Use multiple selectors for precision.\n",
    "  - Example: `div.container > p.text` selects `<p>` tags with `class=\"text\"` that are direct children of a `<div>` with `class=\"container\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World HTML Example\n",
    "```html\n",
    "<div id=\"main\">\n",
    "  <h2 class=\"title\">Article Title</h2>\n",
    "  <p class=\"author\">By John Doe</p>\n",
    "  <ul class=\"tags\">\n",
    "    <li>Python</li>\n",
    "    <li>Web Scraping</li>\n",
    "    <li>Data Science</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d129752-1680-40be-be4a-cdd50074416f",
   "metadata": {},
   "source": [
    "# 3. Static Web Scraping\n",
    "\n",
    "- **`requests`**: A library for making `HTTP requests` to fetch webpage content.\n",
    "    - **HTTP Request**: A communication method used by a client (e.g., your Python script) to interact with a server, asking it to send back specific data or perform an action. Common HTTP request methods include:\n",
    "        - **GET**: Retrieve data from a server (e.g., fetch a webpage).\n",
    "        - **POST**: Submit data to a server (e.g., send form data).\n",
    "  - **HTTP Status Codes**: Indicate the result of the request:\n",
    "    - `200`: Success – The request was successful, and the content is available.\n",
    "    - `404`: Not Found – The requested resource could not be found.\n",
    "    - `500`: Server Error – The server encountered an error.\n",
    "- **`Beautiful Soup`**: A library for `parsing HTML and XML documents`, making it easy to extract and navigate webpage elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11308cb7-0df6-4ae6-a613-e1efc082a2b6",
   "metadata": {},
   "source": [
    "### Example 1: Collect information about countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ded4c8-9630-41a1-8e0d-90fad4f386c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b632db6-1258-4f5f-8d3a-bd2c176632b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the specified URL\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d0f2e5-320b-41a7-a135-ca6b25b47ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status code\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5d7622-ade8-41d3-b3b0-12915eb14982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fe6bee-ebc6-4a31-8816-7ed0b8dc4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all country blocks in the page\n",
    "country_elements = soup.find_all('div', class_='col-md-4 country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3010790b-1159-4832-989c-11f8cc0dc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "\n",
    "# Iterate over each country block\n",
    "for country in country_elements:\n",
    "    # Extract the country name\n",
    "    name = country.find('h3', class_='country-name').get_text(strip=True)\n",
    "    # Extract the capital\n",
    "    capital = country.find('span', class_='country-capital').get_text(strip=True)\n",
    "    # Extract the population\n",
    "    population = country.find('span', class_='country-population').get_text(strip=True)\n",
    "    # Extract the area\n",
    "    area = country.find('span', class_='country-area').get_text(strip=True)\n",
    "    # Save extracted info into a dictionary\n",
    "    country_item = {\"name\":name, \"capital\": capital, \"population\": population, \"area\": area}\n",
    "    # Append dictionary to countries list\n",
    "    countries.append(country_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0e312e-1e9e-4280-a4f1-1e47e0805011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from countries list\n",
    "countries_df = pd.DataFrame(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb826d8-18e8-4200-8de1-2073cb979eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>84000</td>\n",
       "      <td>468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>4975593</td>\n",
       "      <td>82880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>29121286</td>\n",
       "      <td>647500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name           capital population      area\n",
       "0               Andorra  Andorra la Vella      84000     468.0\n",
       "1  United Arab Emirates         Abu Dhabi    4975593   82880.0\n",
       "2           Afghanistan             Kabul   29121286  647500.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display countries DataFrame\n",
    "countries_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c763a4c1-026a-4b06-ba8a-b56d99191ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save countries DataFrame as Excel\n",
    "#countries_df.to_excel(\"countries.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97250f42-8a91-4e9a-a737-feb189f59c64",
   "metadata": {},
   "source": [
    "# 4. Dynamic Web Scraping\n",
    "\n",
    "Some webpages are rendered dynamically using JavaScript, meaning the content may not be fully loaded in the HTML source code fetched by `requests`. In such cases, we can use tools like **`Selenium`**` to interact with and scrape these pages.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to JavaScript-Rendered Pages\n",
    "- Unlike static pages, dynamic pages use JavaScript to load or modify content after the initial HTML is loaded.\n",
    "- Example: Data tables that load as you scroll or elements that appear after clicking a button.\n",
    "\n",
    "---\n",
    "\n",
    "## Using Selenium for Web Scraping\n",
    "- **Selenium**: A Python library that allows automation of web browsers for tasks like interacting with JavaScript-rendered pages.\n",
    "- **WebDriver**: A tool within Selenium that acts as a bridge between Python and the browser.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Selenium with Chrome\n",
    "- Option 1: Manually Set Up Chrome WebDriver\n",
    "- Option 2: Automatically Set Up Chrome WebDriver (`webdriver_manager`)\n",
    "\n",
    "### Option 1: Manually Set Up Chrome WebDriver\n",
    "\n",
    "To manually set up the Chrome WebDriver, follow these steps:\n",
    "\n",
    "1. Go to the [Chrome for Testing Downloads](https://googlechromelabs.github.io/chrome-for-testing/).\n",
    "2. Select the appropriate file for your operating system (e.g., Windows, macOS, Linux).\n",
    "3. Download and unzip the file.\n",
    "4. Note the path to the unzipped `chromedriver` executable.\n",
    "5. Update your script to set the `Service` parameter with the path to the executable:\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "chrome_options = Options()\n",
    "\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.maximize_window()\n",
    "```\n",
    "\n",
    "### Option 2: Automatically Set Up Chrome WebDriver (Recommended)\n",
    "\n",
    "To automatically set up the Chrome WebDriver, follow these steps:\n",
    "\n",
    "1. Install the `webdriver_manager` library:\n",
    "   ```bash\n",
    "   pip install webdriver_manager\n",
    "   ```\n",
    "2. Import ChromeDriverManager from webdriver_manager\n",
    "3. Set the `Service` parameter using ChromeDriverManager:\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up Chrome WebDriver using webdriver_manager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6f1ac-81f4-41c8-8b01-98242bd0be01",
   "metadata": {},
   "source": [
    "### Example 2: Collect Oscar Winning Films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e24a0a-ad63-47d6-98fa-381db4c9ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e397ee-dd5a-45b4-96dd-ee7dbc5e0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the WebDriver\n",
    "chrome_options = Options()\n",
    "\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d0885b-087e-4c62-87d5-833329c8f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the webpage\n",
    "url = \"https://www.scrapethissite.com/pages/ajax-javascript/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f544f7-8604-490a-88f5-904394d86184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the page to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Find all year buttons\n",
    "# Option 1: Find items by id attribute\n",
    "year_ids = [\"2015\", \"2014\", \"2013\", \"2012\", \"2011\", \"2010\"]\n",
    "year_buttons = [driver.find_element(By.ID, year_id) for year_id in year_ids]\n",
    "\n",
    "# Option 2: Find items by class attribute\n",
    "year_buttons = driver.find_elements(By.CLASS_NAME, \"year-link\")\n",
    "\n",
    "# Option 3 (advanced): Find items by XPATH or CSS Selector\n",
    "year_buttons = driver.find_elements(By.XPATH, \"//a[@class='year-link']\")\n",
    "year_buttons = driver.find_elements(By.CSS_SELECTOR, \".year-link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b40494f1-42fa-4929-98a2-ca0089623243",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = []\n",
    "\n",
    "# Loop through each year button\n",
    "for button in year_buttons:\n",
    "    # Click the year button\n",
    "    button.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Wait for the movies table to load\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"table\")))\n",
    "    \n",
    "    # Extract movie details\n",
    "    movie_elements = driver.find_element(By.TAG_NAME, \"tbody\").find_elements(By.TAG_NAME, \"tr\")\n",
    "    for movie in movie_elements:\n",
    "        title = movie.find_element(By.CLASS_NAME, \"film-title\").text\n",
    "        nominations = movie.find_element(By.CLASS_NAME, \"film-nominations\").text\n",
    "        awards = movie.find_element(By.CLASS_NAME, \"film-awards\").text\n",
    "        best_picture = \"Yes\" if movie.find_element(By.CLASS_NAME, \"film-best-picture\").find_elements(By.TAG_NAME, \"i\") else \"No\"\n",
    "        \n",
    "        # Append to the movies list\n",
    "        movies.append({\n",
    "            \"title\": title,\n",
    "            \"nominations\": nominations,\n",
    "            \"awards\": awards,\n",
    "            \"best_picture\": best_picture\n",
    "        })\n",
    "    \n",
    "    # Wait a moment before clicking the next button (to prevent issues with JavaScript execution)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a40306-a04d-44ce-98e3-9250a10c28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from collected movies list\n",
    "movies_df = pd.DataFrame(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96ce14b5-04ad-468b-872e-1a5f055b8106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>nominations</th>\n",
       "      <th>awards</th>\n",
       "      <th>best_picture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mad Max: Fury Road</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Revenant</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title nominations awards best_picture\n",
       "0           Spotlight           6      2          Yes\n",
       "1  Mad Max: Fury Road          10      6           No\n",
       "2        The Revenant          12      3           No"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display movies DataFrame\n",
    "movies_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fecd05-557c-471c-a814-954b3b504cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save movies DataFrame as Excel\n",
    "#movies_df.to_excel(\"movies.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71742a10-efce-4500-b90d-810bb6e40d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33ca1a-7310-47ac-8e51-cb7aac3f1fe6",
   "metadata": {},
   "source": [
    "### Example 3: Collect Hockey Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381aa438-4c91-4455-b15e-4be56c82e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "729e423f-be3c-435e-9244-37796428d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up driver\n",
    "chrome_options = Options()\n",
    "\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b489f25b-22b6-42a5-ba33-01f4aebbc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the webpage\n",
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90337393-0df4-4a7c-b627-14850d317e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Interact with the input search box and search for a team\n",
    "search_box = wait.until(EC.presence_of_element_located((By.ID, \"q\")))\n",
    "search_box.send_keys(\"Pittsburgh Penguins\")  # Enter the team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29611bdc-5fcc-445a-b800-9e5545c3dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element(By.CLASS_NAME, \"btn.btn-primary\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adca9fb1-5207-400f-9652-71794fb9d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handle pagination by clicking on the \"Next\" button\n",
    "driver.get(url)\n",
    "pagination = driver.find_element(By.CLASS_NAME, \"pagination\")\n",
    "pagination_items = pagination.find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "next_button = driver.find_element(By.XPATH, \"//a[@aria-label='Next']\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8886e340-bea3-4c55-a8f2-4ec7dabb248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select an option from the dropdown menu (\"Per Page\")\n",
    "dropdown = Select(wait.until(EC.presence_of_element_located((By.ID, \"per_page\"))))\n",
    "dropdown.select_by_visible_text(\"50\")  # Select \"50 per page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaefc2b2-787f-4959-a058-5193fa05b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scroll down and up\n",
    "# Scroll down the page\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Wait for a moment to observe the scroll effect\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67c11ba4-9a48-48ba-b54e-1bcdc2a5e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll up the page\n",
    "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "# Wait again to observe the scroll effect\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca756510-3ca2-418d-a565-cb50e600b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll down by 500 pixels\n",
    "driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "\n",
    "# Wait for observation\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f179e46e-fb23-41bc-80d1-8bbc6ba08157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll up by 300 pixels\n",
    "driver.execute_script(\"window.scrollBy(0, -300);\")\n",
    "\n",
    "# Wait for observation\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b25a1636-80c1-4ebd-94c0-9d84fbb76948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6a8e8-f545-4244-ba96-b70d757624f4",
   "metadata": {},
   "source": [
    "# 5. Further Learning\n",
    "\n",
    "To deepen your understanding of web scraping and related topics, here are some suggestions:\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1. Understanding APIs\n",
    "- Learn to use APIs for data collection when available, as an alternative to web scraping.\n",
    "  - Examples: Twitter API, Reddit API, Spotify API\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2. Inspecting Network Requests\n",
    "- Use the **Network** tab in the browser's developer tools to monitor and analyze network activity.\n",
    "  - **AJAX Requests**: Identify dynamic data-loading requests.\n",
    "  - **Request/Response Headers**: Understand the information exchanged between client and server.\n",
    "  - **Filtering**: Use filters like `XHR`, `JS`, or `Doc` to isolate specific requests.\n",
    "  - **Preview/Response Tab**: Examine JSON or other data returned by APIs.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3. Handling Advanced Web Scraping Challenges\n",
    "- **Pagination**: Automate scraping across multi-page websites using \"Next\" buttons or page links.\n",
    "- **Infinite Scrolling**: Learn to simulate scrolling with Selenium or fetch data using AJAX requests.\n",
    "- **CAPTCHA Handling**: While ethically debatable, understand basic approaches and alternatives like manual CAPTCHA solving.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4. Deploying Scraping Scripts\n",
    "- Automate scraping tasks using schedulers:\n",
    "  - **Linux**: Use `cron` jobs.\n",
    "  - **Windows**: Use Task Scheduler.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5. Debugging and Logging\n",
    "- **Debugging Tools**:\n",
    "  - Use browser developer tools to troubleshoot issues in the webpage structure.\n",
    "  - Leverage Python debugging tools like `pdb` for your scripts.\n",
    "- **Logging**:\n",
    "  - Add logs to your scripts using Python’s `logging` module to monitor scraping activity and identify errors.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
